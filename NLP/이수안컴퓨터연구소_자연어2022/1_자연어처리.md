

[toc]







## 자연어 처리 단계

1. 어휘 분석(Lexical Analysis): 단어의 구조를 식별, 형태소(의미)와 품사 파악

   - 형태소 분석(Morphological Analysis)
     - 더 이상 분해될 수 없는 최소한의 의미를 갖는 단위인 형태소를 사용해 단어가 어떻게 형성되는지에 대해 자연어의 제약 조건과 문법 규칙에 맞춰 분석

   - 품사 태깅(Part-of-Speech Tagging)
     -  단어의 기능, 형태, 의미에 따라 나눈 것이 품사이고, 같은 단어에 대해 의미가 다를 경우(중의성)를 해결하기 위해 **부가적인 언어의 정보를 부착하는 태깅(단어마다 품사 태깅)**

2. 구문 분석(Syntactic Analysis): **단어들의 관계 구조화**(문법적 구조 분석)

   - 구구조 구문 분석(Phrase Structure Parsing)
     - 구의 구조 문법에 기반한 구문 분석 기술

   - 의존 구문 분석(Dependency Parsing)
     - 자연어 문장에서 단어 간의 의존 관계를 분석함으로써 문장 전체의 문법적 구조를 분석

3. 의미 분석(Semantic Analysis):  **문장을 해석**

   - **단어 의미 중의성 해소**(Word Sense Disambiguation)
     - 문장 내 중의성을 가지는 어휘를 사전에 정의된 의미와 매칭하여 어휘적 중의성을 해결

   - 의미역(Semantic Role) 분석
     - 의미를 해석하기 위해 서술어가 수식하는 대상의 의미 관계를 파악하고, 그 역할을 분류





## 토큰화(Tokenization)

+ 특수 문자 처리

+ 특정 단어 토큰 분리 방법

  + we're, United Kingdom

+ **하고자하는 처리의 특성에 따라 토큰화 방법 선택**

  



### 단어 토큰화

+ `.split()`
+ `from nltk.tokenize import word_tokenize`
+ `.pos()`
  + 한국어는 조사와 접속사를 분리해주는 방식 사용

+ `.morphs()`
  + 한국어를 형태소로만 구분하길 원하는 경우 사용

+ `.nouns()`
  + 명사만 사용하고 싶을 때 사용



### 문장 토큰화

+ `from nltk.tokenize import sent_tokenize`
+ konlpy의 형태소 분석기 중에서는 `Kkma()`만 가능

+ 한국어 문장을 토큰화할 때는 `kss(korean sentence splitter)` 라이브러리 이용

  ```python
  import kss
  text = '진짜? 내일 뭐하지. 이렇게 애매모호한 문장도? 밥은 먹었어. 나는'
  print(kss.split_sentences(text))
  ```

  

### 정규 표현식을 이용한 토큰화

+ `from nltk.tokenize import RegexpTokenizer`

  + `gaps=True` : 분리 기준을 space

  ```python
  from nltk.tokenize import RegexpTokenizer
  sentence = 'Where there\'s a will, there\'s a way'
  
  # word에 대해서 tokenize + 특수문자 제거
  tokenizer = RegexpTokenizer("[\w]+")
  tokens = tokenizer.tokenize(sentence)
  >>> ['Where', 'there', 's', 'a', 'will', 'there', 's', 'a', 'way']
  
  # 공백에 대해서 tokenize
  tokenizer = RegexpTokenizer("[\s]+", gaps=True) # space 기준으로 tokenize 해줘
  tokens = tokenizer.tokenize(sentence)
  >>> ['Where', "there's", 'a', 'will,', "there's", 'a', 'way']
  ```

  ```python
  
  sentence = '안녕하세요 ㅋㅋ 저는 자연어 처리(Natural Language Processing)를ㄹ!! 배우고 있습니다.'
  
  # 한국어 완성된 글자에 대해서 tokenize
  tokenizer = RegexpTokenizer("[가-힣]+")
  tokens = tokenizer.tokenize(sentence)
  >>> ['안녕하세요', '저는', '자연어', '처리', '를', '배우고', '있습니다']
  
  # 한국어 자음 기준 + space 기준
  tokenizer = RegexpTokenizer("ㄱ-ㅎ]+", gaps=True)
  tokens = tokenizer.tokenize(sentence)
  >>> ['안녕하세요 ㅋㅋ 저는 자연어 처리(Natural Language Processing)를ㄹ!! 배우고 있습니다.']
  ```





### TextBlob을 이용한 토큰화

+ `from textblob import TextBlob`

  ```python
  from textblob import TextBlob
  
  # 영어
  eng = "Where there\'s a will. there\'s a way"
  blob = TextBlob(eng)
  blob.words
  >>> WordList(['Where', 'there', "'s", 'a', 'will', 'there', "'s", 'a', 'way'])
  
  # 한국어
  kor = '성공의 비결은 단 한 가지, 잘할 수 있는 일에 광적으로 집중하는 것이다.'
  blob = TextBlob(kor)
  blob.words
  >>> WordList(['성공의', '비결은', '단', '한', '가지', '잘할', '수', '있는', '일에', '광적으로', '집중하는', '것이다'])
  ```



### 케라스를 이용한 토큰화

+ `from keras.preprocessing.text import text_to_word_sequence`

  ```python
  from keras.preprocessing.text import text_to_word_sequence
  text_to_word_sequence(eng)
  >>> ['where', "there's", 'a', 'will', "there's", 'a', 'way']
  text_to_word_sequence(kor)
  >>> ['성공의', '비결은', '단', '한', '가지', '잘할', '수', '있는', '일에', '광적으로', '집중하는', '것이다']
  ```

  



### 기타 토크나이저

+  `WhiteSpaceTokenizer`: 공백을 기준으로 토큰화

+ `WordPunktTokenizer`: 텍스트를 알파벳 문자, 숫자, 알파벳 이외의 문자 리스트로 토큰화

+ `MWETokenizer`: MWE는 Multi-Word Expression의 약자로 **'republic of korea'와 같이 여러 단어로 이뤄진 특정 그룹을 한 개체**로 취급

+ `TweetTokenizer`: 트위터에서 사용되는 문장의 토큰화를 위해서 만들어졌으며, **문장 속 감성의 표현과 감정**을 다룸









## n-gram

+ bigram(n=2)가 가장 많이 쓰임

+ `from nltk import ngrmas`

  ```python
  from nltk import ngrams
  
  sentence = 'There is no royal road to learning'
  bigram = list(ngrams(sentence.split(),2))
  print(bigram)
  >>> [('There', 'is'), ('is', 'no'), ('no', 'royal'), ('royal', 'road'), ('road', 'to'), ('to', 'learning')]
  
  ```



+ `from textblob import TextBlob`

  ```python
  from textblob import TextBlob
  blob = TextBlob(sentence)
  blob.ngrams(n=3)
  ```

  





## PoS 태깅 (품사태깅)

### 패키지

+ `from nltk.tokenize import word_tokenize` & `nltk.pos_tag()`

  ```python
  from nltk.tokenize import word_tokenize
  sentence = 'Think like man of action and act like man of thought'
  words = word_tokenize(sentence)
  >>> ['Think',
   'like',
   'man',
   'of',
   'action',
   'and',
   'act',
   'like',
   'man',
   'of',
   'thought']
  
  nltk.download('averaged_perceptron_tagger')
  nltk.pos_tag(words)
  [('Think', 'VBP'),
   ('like', 'IN'),
   ('man', 'NN'),
   ('of', 'IN'),
   ('action', 'NN'),
   ('and', 'CC'),
   ('act', 'NN'),
   ('like', 'IN'),
   ('man', 'NN'),
   ('of', 'IN'),
   ('thought', 'NN')]
  ```



### 태그 리스트

| Number | Tag | Description | 설명 |
| -- | -- | -- | -- |
| 1 | `CC` | Coordinating conjunction |
| 2 | `CD` | Cardinal number |
| 3 | `DT` | Determiner | 한정사
| 4 | `EX` | Existential there |
| 5 | `FW` | Foreign word | 외래어 |
| 6 | `IN` | Preposition or subordinating conjunction | 전치사 또는 종속 접속사 |
| 7 | `JJ` | Adjective | 형용사 |
| 8 | `JJR` | Adjective, comparative | 헝용사, 비교급 |
| 9 | `JJS` | Adjective, superlative | 형용사, 최상급 |
| 10 | `LS` | List item marker |
| 11 | `MD` | Modal |
| 12 | `NN` | Noun, singular or mass | 명사, 단수형 |
| 13 | `NNS` | Noun, plural | 명사, 복수형 |
| 14 | `NNP` | Proper noun, singular | 고유명사, 단수형 |
| 15 | `NNPS` | Proper noun, plural | 고유명사, 복수형 |
| 16 | `PDT` | Predeterminer | 전치한정사 |
| 17 | `POS` | Possessive ending | 소유형용사 |
| 18 | `PRP` | Personal pronoun | 인칭 대명사 |
| 19 | `PRP$` | Possessive pronoun | 소유 대명사 |
| 20 | `RB` | Adverb | 부사 |
| 21 | `RBR` | Adverb, comparative | 부사, 비교급 |
| 22 | `RBS` | Adverb, superlative | 부사, 최상급 |
| 23 | `RP` | Particle |
| 24 | `SYM` | Symbol | 기호
| 25 | `TO` | to |
| 26 | `UH` | Interjection | 감탄사 |
| 27 | `VB` | Verb, base form | 동사, 원형 |
| 28 | `VBD` | Verb, past tense | 동사, 과거형 |
| 29 | `VBG` | Verb, gerund or present participle | 동사, 현재분사 |
| 30 | `VBN` | Verb, past participle | 동사, 과거분사 |
| 31 | `VBP` | Verb, non-3rd person singular present | 동사, 비3인칭 단수 |
| 32 | `VBZ` | Verb, 3rd person singular present | 동사, 3인칭 단수 |
| 33 | `WDT` | Wh-determiner |
| 34 | `WP` | Wh-pronoun |
| 35 | `WP$` | Possessive wh-pronoun |
| 36 | `WRB` | Wh-adverb |





## 불용어 (Stopword) 제거

+ 영어의 전치사나 한국어의 조사 등 분석에 필요하지 않은 경우가 많음



+ `from nltk.corpus import stopwords`

  ```python
  from nltk.corpus import stopwords
  nltk.download('stopwords')
  stop_words = stopwords.words('english')
  print(stop_words)
  >>> ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"]
  
  ```

  



## 철자 교정(Spelling Correction)

+ 오타가 있다면, counting이 제대로 안되는 문제 발생



+ `!pip install autocorrect`

  + `from autocorrect import Speller`

    ```python
    from autocorrect import Speller
    
    spell = Speller('en')
    
    print(spell('peoplle'))
    print(spell('peope'))
    print(spell('peopae'))
    ```

    ```python
    # 실 사용 예
    from nltk.tokenize import word_tokenize
    tokens = word_tokenize('Earlly biird catchess the womm.')
    print(tokens)
    >>> ['Earlly', 'biird', 'catchess', 'the', 'womm', '.']
    
    corrected_sentence = ' '.join([spell(token) for token in tokens])
    print(corrected_sentence)
    >>> Early bird catches the worm .
    ```

    



## 단수화와 복수화

+ word count 할 때, 단수화해서 사용하기도 함

  

+ `from textblob import TextBlob`

  + `textblob.words.singularize()`

  + `textblob.words.pluralize()`

    ```python
    # 단수화
    from textblob import TextBlob
    
    words = 'apples bananas oranges'
    textblob = TextBlob(words)
    
    print(textblob.words)
    print(textblob.words.singularize())
    
    # 복수화
    words = 'car, train ariplane'
    textblob = TextBlob(words)
    
    print(textblob.words)
    print(textblob.words.pluralize())
    ```

    



### 어간 추출

+ `stemmer = nltk.stem.PorterStemmer()`

  ```python
  stemmer = nltk.stem.PorterStemmer()
  stemmer.stem('application')
  ```

  



### 표제어 추출

+ ```python
  from nltk.stem.wordnet import WordNetLemmatizer
  
  nltk.download('wordnet')
  nltk.download('omw-1.4')
  lemmatizer = WordNetLemmatizer()
  lemmatizer.lemmatize('application')
  ```

  + `wordnet`이 표제어에 대한 정보를 담고 있음











### 다운로드 목록

```python

# 표제어 추출
from nltk.stem.wordnet import WordNetLemmatizer
nltk.download('wordnet')
nltk.download('omw-1.4')

# 개체명 인식
nltk.download('maxent_ne_chunker')
nltk.download('words')
nltk.download('averaged_perceptron_tagger')
```














## 자연어 처리 단계

1. 어휘 분석(Lexical Analysis): 단어의 구조를 식별, 형태소(의미)와 품사 파악

   - 형태소 분석(Morphological Analysis)
     - 더 이상 분해될 수 없는 최소한의 의미를 갖는 단위인 형태소를 사용해 단어가 어떻게 형성되는지에 대해 자연어의 제약 조건과 문법 규칙에 맞춰 분석

   - 품사 태깅(Part-of-Speech Tagging)
     -  단어의 기능, 형태, 의미에 따라 나눈 것이 품사이고, 같은 단어에 대해 의미가 다를 경우(중의성)를 해결하기 위해 **부가적인 언어의 정보를 부착하는 태깅(단어마다 품사 태깅)**

2. 구문 분석(Syntactic Analysis): **단어들의 관계 구조화**(문법적 구조 분석)

   - 구구조 구문 분석(Phrase Structure Parsing)
     - 구의 구조 문법에 기반한 구문 분석 기술

   - 의존 구문 분석(Dependency Parsing)
     - 자연어 문장에서 단어 간의 의존 관계를 분석함으로써 문장 전체의 문법적 구조를 분석

3. 의미 분석(Semantic Analysis):  **문장을 해석**

   - **단어 의미 중의성 해소**(Word Sense Disambiguation)
     - 문장 내 중의성을 가지는 어휘를 사전에 정의된 의미와 매칭하여 어휘적 중의성을 해결

   - 의미역(Semantic Role) 분석
     - 의미를 해석하기 위해 서술어가 수식하는 대상의 의미 관계를 파악하고, 그 역할을 분류





## 토큰화(Tokenization)

+ 특수 문자 처리

+ 특정 단어 토큰 분리 방법

  + we're, United Kingdom

+ **하고자하는 처리의 특성에 따라 토큰화 방법 선택**

  



### 단어 토큰화

+ `.split()`
+ `from nltk.tokenize import word_tokenize`
+ `.pos()`
  + 한국어는 조사와 접속사를 분리해주는 방식 사용

+ `.morphs()`
  + 한국어를 형태소로만 구분하길 원하는 경우 사용

+ `.nouns()`
  + 명사만 사용하고 싶을 때 사용



### 문장 토큰화

+ `from nltk.tokenize import sent_tokenize`
+ konlpy의 형태소 분석기 중에서는 `Kkma()`만 가능

+ 한국어 문장을 토큰화할 때는 `kss(korean sentence splitter)` 라이브러리 이용

  ```python
  import kss
  text = '진짜? 내일 뭐하지. 이렇게 애매모호한 문장도? 밥은 먹었어. 나는'
  print(kss.split_sentences(text))
  ```

  

### 정규 표현식을 이용한 토큰화

+ `from nltk.tokenize import RegexpTokenizer`

  + `gaps=True` : 분리 기준을 space

  ```python
  from nltk.tokenize import RegexpTokenizer
  sentence = 'Where there\'s a will, there\'s a way'
  
  # word에 대해서 tokenize + 특수문자 제거
  tokenizer = RegexpTokenizer("[\w]+")
  tokens = tokenizer.tokenize(sentence)
  >>> ['Where', 'there', 's', 'a', 'will', 'there', 's', 'a', 'way']
  
  # 공백에 대해서 tokenize
  tokenizer = RegexpTokenizer("[\s]+", gaps=True) # space 기준으로 tokenize 해줘
  tokens = tokenizer.tokenize(sentence)
  >>> ['Where', "there's", 'a', 'will,', "there's", 'a', 'way']
  ```

  ```python
  
  sentence = '안녕하세요 ㅋㅋ 저는 자연어 처리(Natural Language Processing)를ㄹ!! 배우고 있습니다.'
  
  # 한국어 완성된 글자에 대해서 tokenize
  tokenizer = RegexpTokenizer("[가-힣]+")
  tokens = tokenizer.tokenize(sentence)
  >>> ['안녕하세요', '저는', '자연어', '처리', '를', '배우고', '있습니다']
  
  # 한국어 자음 기준 + space 기준
  tokenizer = RegexpTokenizer("ㄱ-ㅎ]+", gaps=True)
  tokens = tokenizer.tokenize(sentence)
  >>> ['안녕하세요 ㅋㅋ 저는 자연어 처리(Natural Language Processing)를ㄹ!! 배우고 있습니다.']
  ```





### TextBlob을 이용한 토큰화

+ `from textblob import TextBlob`

  ```python
  from textblob import TextBlob
  
  # 영어
  eng = "Where there\'s a will. there\'s a way"
  blob = TextBlob(eng)
  blob.words
  >>> WordList(['Where', 'there', "'s", 'a', 'will', 'there', "'s", 'a', 'way'])
  
  # 한국어
  kor = '성공의 비결은 단 한 가지, 잘할 수 있는 일에 광적으로 집중하는 것이다.'
  blob = TextBlob(kor)
  blob.words
  >>> WordList(['성공의', '비결은', '단', '한', '가지', '잘할', '수', '있는', '일에', '광적으로', '집중하는', '것이다'])
  ```



### 케라스를 이용한 토큰화

+ `from keras.preprocessing.text import text_to_word_sequence`

  ```python
  from keras.preprocessing.text import text_to_word_sequence
  text_to_word_sequence(eng)
  >>> ['where', "there's", 'a', 'will', "there's", 'a', 'way']
  text_to_word_sequence(kor)
  >>> ['성공의', '비결은', '단', '한', '가지', '잘할', '수', '있는', '일에', '광적으로', '집중하는', '것이다']
  ```

  



### 기타 토크나이저

+  `WhiteSpaceTokenizer`: 공백을 기준으로 토큰화

+ `WordPunktTokenizer`: 텍스트를 알파벳 문자, 숫자, 알파벳 이외의 문자 리스트로 토큰화

+ `MWETokenizer`: MWE는 Multi-Word Expression의 약자로 **'republic of korea'와 같이 여러 단어로 이뤄진 특정 그룹을 한 개체**로 취급

+ `TweetTokenizer`: 트위터에서 사용되는 문장의 토큰화를 위해서 만들어졌으며, **문장 속 감성의 표현과 감정**을 다룸

